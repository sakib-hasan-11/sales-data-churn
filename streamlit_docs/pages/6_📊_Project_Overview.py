"""
Page 6: Project Overview
Complete project structure and flow documentation
"""

import streamlit as st

st.set_page_config(page_title="Project Overview", page_icon="ğŸ“Š", layout="wide")

# Custom CSS
st.markdown(
    """
<style>
    .section-header {
        font-size: 1.8rem;
        font-weight: bold;
        color: #6A1B9A;
        margin-top: 2rem;
        background-color: #F3E5F5;
        padding: 1rem;
        border-radius: 8px;
        border-left: 5px solid #6A1B9A;
    }
    .module-box {
        background-color: #E8EAF6;
        padding: 1.5rem;
        border-radius: 8px;
        margin: 1rem 0;
        border-left: 4px solid #3F51B5;
    }
    .flow-box {
        background-color: #E0F2F1;
        padding: 1.5rem;
        border-radius: 8px;
        margin: 1rem 0;
        border-left: 4px solid #00897B;
    }
    .file-tree {
        font-family: 'Courier New', monospace;
        background-color: #263238;
        color: #AED581;
        padding: 1.5rem;
        border-radius: 8px;
        font-size: 0.9rem;
        line-height: 1.6;
        overflow-x: auto;
    }
    .highlight {
        background-color: #FFECB3;
        padding: 2px 6px;
        border-radius: 3px;
        font-weight: bold;
    }
</style>
""",
    unsafe_allow_html=True,
)

# Header
st.title("ğŸ“Š Project Overview")
st.markdown("### Complete End-to-End ML Project Structure")
st.markdown("---")

# Project Summary
st.markdown("""
## ğŸ¯ Project Summary

**Goal**: Predict customer churn with production-ready ML pipeline

**Business Value**:
- ğŸ“‰ Reduce customer churn by identifying at-risk customers
- ğŸ’° Enable proactive retention campaigns
- ğŸ“Š Optimize marketing spend with targeted interventions

**Tech Stack**:
- **ML**: XGBoost, Scikit-learn, Optuna
- **Tracking**: MLflow
- **API**: FastAPI, Uvicorn
- **Deployment**: Docker, AWS ECS
- **Data**: Pandas, NumPy
- **Validation**: Great Expectations
""")

st.markdown("---")

# Complete File Structure
st.markdown(
    '<div class="section-header">ğŸ“ Complete File Structure</div>',
    unsafe_allow_html=True,
)

st.markdown(
    """
<div class="file-tree">
sales-data-churn/
â”‚
â”œâ”€â”€ <span style="color: #80CBC4;">ğŸ“„ main.py</span>                    # FastAPI application (production API)
â”œâ”€â”€ <span style="color: #80CBC4;">ğŸ“„ app.py</span>                     # Streamlit dashboard (existing)
â”œâ”€â”€ <span style="color: #80CBC4;">ğŸ“„ requirements.txt</span>           # Python dependencies
â”œâ”€â”€ <span style="color: #80CBC4;">ğŸ“„ Dockerfile</span>                 # Container image definition
â”œâ”€â”€ <span style="color: #80CBC4;">ğŸ“„ docker-compose.yml</span>         # Local Docker environment
â”œâ”€â”€ <span style="color: #80CBC4;">ğŸ“„ .dockerignore</span>              # Docker build exclusions
â”œâ”€â”€ <span style="color: #80CBC4;">ğŸ“„ .env.example</span>               # Configuration template
â”œâ”€â”€ <span style="color: #80CBC4;">ğŸ“„ ecs-task-definition.json</span>  # AWS ECS deployment config
â”œâ”€â”€ <span style="color: #80CBC4;">ğŸ“„ DEPLOYMENT.md</span>              # Deployment guide
â”œâ”€â”€ <span style="color: #80CBC4;">ğŸ“„ API_EXAMPLES.md</span>            # API usage examples
â”œâ”€â”€ <span style="color: #80CBC4;">ğŸ“„ test_api.py</span>                # API test suite
â”‚
â”œâ”€â”€ <span style="color: #FFD54F;">ğŸ“‚ data/</span>                      # Data storage
â”‚   â”œâ”€â”€ raw/                  # Original datasets
â”‚   â”‚   â”œâ”€â”€ train.csv
â”‚   â”‚   â”œâ”€â”€ test.csv
â”‚   â”‚   â””â”€â”€ holdout.csv
â”‚   â””â”€â”€ processed/            # Preprocessed datasets
â”‚       â”œâ”€â”€ train_processed.csv
â”‚       â”œâ”€â”€ test_processed.csv
â”‚       â””â”€â”€ holdout_processed.csv
â”‚
â”œâ”€â”€ <span style="color: #FFD54F;">ğŸ“‚ src/</span>                       # Source code (modular)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ <span style="color: #81C784;">ğŸ“‚ data_processing/</span>    # Data loading & cleaning
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ <span style="color: #AED581;">load.py</span>           # Load CSV data
â”‚   â”‚   â””â”€â”€ <span style="color: #AED581;">preprocess.py</span>     # Clean & handle missing values
â”‚   â”‚
â”‚   â”œâ”€â”€ <span style="color: #81C784;">ğŸ“‚ features/</span>           # Feature engineering
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ <span style="color: #AED581;">build_feature.py</span>  # Create 10 new features
â”‚   â”‚   â””â”€â”€ <span style="color: #AED581;">feature_preprocess.py</span> # Encode & scale
â”‚   â”‚
â”‚   â”œâ”€â”€ <span style="color: #81C784;">ğŸ“‚ training/</span>           # Model training
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ <span style="color: #AED581;">optuna_tuning.py</span>  # Hyperparameter optimization
â”‚   â”‚   â”œâ”€â”€ <span style="color: #AED581;">mlflow_training.py</span> # MLflow experiment tracking
â”‚   â”‚   â””â”€â”€ <span style="color: #AED581;">evaluation.py</span>      # Model evaluation & saving
â”‚   â”‚
â”‚   â”œâ”€â”€ <span style="color: #81C784;">ğŸ“‚ inference/</span>          # Production inference
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ <span style="color: #AED581;">inference.py</span>       # Prediction engine
â”‚   â”‚
â”‚   â””â”€â”€ <span style="color: #81C784;">ğŸ“‚ utils/</span>              # Utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ <span style="color: #AED581;">data_validator.py</span>  # Data quality checks
â”‚
â”œâ”€â”€ <span style="color: #FFD54F;">ğŸ“‚ scripts/</span>                   # Pipeline scripts
â”‚   â”œâ”€â”€ run_pipeline.py       # Main training pipeline
â”‚   â”œâ”€â”€ colab_pipeline.py     # Colab-specific pipeline
â”‚   â””â”€â”€ prepare_ci_data.py    # Data preparation
â”‚
â”œâ”€â”€ <span style="color: #FFD54F;">ğŸ“‚ notebooks/</span>                 # Jupyter notebooks
â”‚   â”œâ”€â”€ EDA_and_Feature_Engineering.ipynb
â”‚   â””â”€â”€ tree_based_recall_models.ipynb
â”‚
â”œâ”€â”€ <span style="color: #FFD54F;">ğŸ“‚ models/</span>                    # Saved models (optional)
â”‚
â”œâ”€â”€ <span style="color: #FFD54F;">ğŸ“‚ mlruns/</span>                    # MLflow tracking data
â”‚   â””â”€â”€ <experiment_id>/
â”‚       â””â”€â”€ <run_id>/
â”‚           â”œâ”€â”€ artifacts/    # Model files
â”‚           â”œâ”€â”€ metrics/      # Performance metrics
â”‚           â”œâ”€â”€ params/       # Hyperparameters
â”‚           â””â”€â”€ tags/         # Metadata
â”‚
â”œâ”€â”€ <span style="color: #FFD54F;">ğŸ“‚ outputs/</span>                   # Evaluation results
â”‚   â”œâ”€â”€ holdout_predictions.csv
â”‚   â””â”€â”€ holdout_evaluation_summary.txt
â”‚
â”œâ”€â”€ <span style="color: #FFD54F;">ğŸ“‚ tests/</span>                     # Unit tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_data.py
â”‚
â””â”€â”€ <span style="color: #FFD54F;">ğŸ“‚ streamlit_docs/</span>            # This documentation!
    â”œâ”€â”€ Home.py
    â””â”€â”€ pages/
        â”œâ”€â”€ 1_ğŸ“_Data_Processing.py
        â”œâ”€â”€ 2_ğŸ”§_Feature_Engineering.py
        â”œâ”€â”€ 3_ğŸ“_Model_Training.py
        â”œâ”€â”€ 4_ğŸ”®_Inference_Engine.py
        â”œâ”€â”€ 5_ğŸŒ_API_Deployment.py
        â””â”€â”€ 6_ğŸ“Š_Project_Overview.py
</div>
""",
    unsafe_allow_html=True,
)

st.markdown("---")

# Module Summary
st.markdown(
    '<div class="section-header">ğŸ“¦ Module Summary</div>', unsafe_allow_html=True
)

col1, col2 = st.columns(2)

with col1:
    st.markdown('<div class="module-box">', unsafe_allow_html=True)
    st.markdown(
        """
    ### ğŸ“ Data Processing
    **Files**: 3  
    **Functions**: 3
    
    - `load.py`: Load CSV data
    - `preprocess.py`: Clean & impute
    - `data_validator.py`: Quality checks
    
    **Purpose**: Prepare raw data
    """,
        unsafe_allow_html=True,
    )
    st.markdown("</div>", unsafe_allow_html=True)

    st.markdown('<div class="module-box">', unsafe_allow_html=True)
    st.markdown(
        """
    ### ğŸ“ Model Training
    **Files**: 3  
    **Functions**: 4
    
    - `optuna_tuning.py`: HPO with Optuna
    - `mlflow_training.py`: Track experiments
    - `evaluation.py`: Save best model
    
    **Purpose**: Train & optimize
    """,
        unsafe_allow_html=True,
    )
    st.markdown("</div>", unsafe_allow_html=True)

    st.markdown('<div class="module-box">', unsafe_allow_html=True)
    st.markdown(
        """
    ### ğŸŒ API Deployment
    **Files**: 5  
    **Endpoints**: 6
    
    - `main.py`: FastAPI app
    - `Dockerfile`: Container
    - `docker-compose.yml`: Local test
    - `ecs-task-definition.json`: AWS ECS
    - `.dockerignore`: Optimization
    
    **Purpose**: Production API
    """,
        unsafe_allow_html=True,
    )
    st.markdown("</div>", unsafe_allow_html=True)

with col2:
    st.markdown('<div class="module-box">', unsafe_allow_html=True)
    st.markdown(
        """
    ### ğŸ”§ Feature Engineering
    **Files**: 2  
    **Functions**: 2
    
    - `build_feature.py`: 10 new features
    - `feature_preprocess.py`: Encode & scale
    
    **Purpose**: Create ML features
    """,
        unsafe_allow_html=True,
    )
    st.markdown("</div>", unsafe_allow_html=True)

    st.markdown('<div class="module-box">', unsafe_allow_html=True)
    st.markdown(
        """
    ### ğŸ”® Inference Engine
    **Files**: 1  
    **Classes**: 3  
    **Functions**: 12+
    
    - `inference.py`: Production predictions
      - InferencePreprocessor
      - ModelLoader
      - ChurnPredictor
    
    **Purpose**: Make predictions
    """,
        unsafe_allow_html=True,
    )
    st.markdown("</div>", unsafe_allow_html=True)

st.markdown("---")

# Data Flow
st.markdown(
    '<div class="section-header">ğŸ”„ Complete Data Flow</div>', unsafe_allow_html=True
)

st.markdown('<div class="flow-box">', unsafe_allow_html=True)
st.markdown(
    """
### **Training Pipeline** ğŸ“

```
1ï¸âƒ£ Raw Data (train.csv)
    â”‚
    â”œâ”€> load_data()                    [load.py]
    â”‚
2ï¸âƒ£ Loaded DataFrame
    â”‚
    â”œâ”€> raw_preprocess()               [preprocess.py]
    â”‚   â€¢ Handle missing values
    â”‚   â€¢ Clean outliers
    â”‚
3ï¸âƒ£ Clean DataFrame
    â”‚
    â”œâ”€> build_feature()                [build_feature.py]
    â”‚   â€¢ CLV, support_efficiency
    â”‚   â€¢ payment_reliability, etc.
    â”‚
4ï¸âƒ£ Engineered DataFrame
    â”‚
    â”œâ”€> preprocess_features()          [feature_preprocess.py]
    â”‚   â€¢ Label encode gender
    â”‚   â€¢ One-hot encode categoricals
    â”‚
5ï¸âƒ£ Model-Ready Features (X, y)
    â”‚
    â”œâ”€> tune_hyperparameters()         [optuna_tuning.py]
    â”‚   â€¢ 100 Optuna trials
    â”‚   â€¢ Optimize recall
    â”‚
6ï¸âƒ£ Best Hyperparameters
    â”‚
    â”œâ”€> train_with_mlflow()            [mlflow_training.py]
    â”‚   â€¢ Train XGBoost
    â”‚   â€¢ Log to MLflow
    â”‚
7ï¸âƒ£ Trained Model
    â”‚
    â””â”€> save_model_from_mlflow()       [evaluation.py]
        â€¢ Evaluate on test
        â€¢ Save best model

Result: Model in MLflow (mlruns/)
```
""",
    unsafe_allow_html=True,
)
st.markdown("</div>", unsafe_allow_html=True)

st.markdown('<div class="flow-box">', unsafe_allow_html=True)
st.markdown(
    """
### **Inference Pipeline** ğŸ”®

```
1ï¸âƒ£ New Customer Data (JSON/dict)
    â”‚
    â”œâ”€> FastAPI Endpoint              [main.py]
    â”‚   â€¢ Validate with Pydantic
    â”‚   â€¢ /predict or /predict/batch
    â”‚
2ï¸âƒ£ Validated Data
    â”‚
    â”œâ”€> InferencePreprocessor          [inference.py]
    â”‚   â€¢ clean_column_names()
    â”‚   â€¢ raw_preprocess()
    â”‚   â€¢ build_feature()
    â”‚   â€¢ encode_features()
    â”‚   â€¢ align_features()
    â”‚
3ï¸âƒ£ Preprocessed Features (X)
    â”‚
    â”œâ”€> ChurnPredictor                 [inference.py]
    â”‚   â€¢ model.predict_proba(X)
    â”‚   â€¢ Apply threshold
    â”‚   â€¢ Calculate risk level
    â”‚
4ï¸âƒ£ Prediction Result
    â”‚
    â””â”€> FastAPI Response               [main.py]
        â€¢ Format as JSON
        â€¢ Return to client

Result: {
  "churn_probability": 0.75,
  "churn_prediction": 1,
  "risk_level": "High"
}
```
""",
    unsafe_allow_html=True,
)
st.markdown("</div>", unsafe_allow_html=True)

st.markdown("---")

# Feature Engineering Flow
st.markdown(
    '<div class="section-header">ğŸ”§ Feature Engineering Details</div>',
    unsafe_allow_html=True,
)

st.markdown("""
### **10 Engineered Features**

| Feature | Formula | Business Logic |
|---------|---------|----------------|
| 1. **CLV** | `total_spend / tenure` | Customer Lifetime Value - spending rate |
| 2. **support_efficiency** | `support_calls / max(usage_frequency, 1)` | Support needed per usage |
| 3. **payment_reliability** | `1 / (1 + payment_delay)` | On-time payment score |
| 4. **engagement_score** | `usage_frequency / (1 + last_interaction)` | Recent activity level |
| 5. **value_to_company** | `total_spend / (1 + support_calls)` | Revenue vs support cost |
| 6. **normalized_tenure** | `tenure / max(tenure)` | Relative customer age |
| 7. **days_since_last_interaction** | `last_interaction` | Customer engagement metric |
| 8. **Tenure Category** | Bins: 0-12M, 12-24M, 24-36M, 36-48M, 48M+ | Customer lifecycle stage |
| 9. **Age Group** | Bins: 18-30, 30-40, 40-50, 50-60, 60+ | Demographic segment |
| 10. **Spend Category** | Bottom 33% (Low), Mid 34% (Medium), Top 33% (High) | Spending tier |

**Result**: Original 11 columns â†’ 45+ features after encoding!
""")

st.markdown("---")

# Model Performance
st.markdown(
    '<div class="section-header">ğŸ“ˆ Model Performance</div>', unsafe_allow_html=True
)

col1, col2, col3, col4 = st.columns(4)

with col1:
    st.metric("ğŸ¯ Recall", "~95%", help="Catches 95% of churners")
with col2:
    st.metric("âœ… Precision", "~40-50%", help="Half of predictions are true")
with col3:
    st.metric("ğŸ“Š F1 Score", "~56%", help="Harmonic mean")
with col4:
    st.metric("ğŸ” AUC", "~70%", help="Overall discrimination")

st.markdown("""
**Why Optimize Recall?**
- Business prefers catching all churners (even with false alarms)
- Cost of losing a customer > Cost of retention offer to loyal customer
- Better to be safe than sorry!

**Trade-off**: Higher recall â†’ Lower precision (more false positives)
""")

st.markdown("---")

# Quick Reference
st.markdown(
    '<div class="section-header">ğŸš€ Quick Reference Guide</div>', unsafe_allow_html=True
)

tab1, tab2, tab3, tab4 = st.tabs(
    ["ğŸ“ Training", "ğŸ”® Inference", "ğŸŒ API", "â˜ï¸ Deployment"]
)

with tab1:
    st.markdown("""
    ### Train New Model
    
    ```python
    # 1. Load and preprocess data
    from src.data_processing.load import load_data
    from src.data_processing.preprocess import raw_preprocess
    from src.features.build_feature import build_feature
    from src.features.feature_preprocess import preprocess_features
    
    df = load_data('data/raw/train.csv')
    df_clean = raw_preprocess(df)
    df_features = build_feature(df_clean)
    X, y, feature_names = preprocess_features(df_features)
    
    # 2. Tune hyperparameters
    from src.training.optuna_tuning import tune_hyperparameters
    
    best_params = tune_hyperparameters(X, y)
    
    # 3. Train with MLflow
    from src.training.mlflow_training import train_with_mlflow
    
    run_id = train_with_mlflow(
        X, y,
        params=best_params,
        experiment_name='My_Experiment'
    )
    
    print(f"Model trained! Run ID: {run_id}")
    ```
    """)

with tab2:
    st.markdown("""
    ### Make Predictions
    
    ```python
    from src.inference.inference import create_predictor_from_mlflow
    
    # Initialize predictor
    predictor = create_predictor_from_mlflow(
        experiment_name='Colab_GPU_Training',
        metric='recall'
    )
    
    # Single prediction
    customer = {
        "customerid": "CUST001",
        "age": 35,
        "gender": "Male",
        "tenure": 24,
        "usage_frequency": 15,
        "support_calls": 3,
        "payment_delay": 5,
        "subscription_type": "Premium",
        "contract_length": "Annual",
        "total_spend": 1250.50,
        "last_interaction": 10
    }
    
    result = predictor.predict(customer)
    print(result)
    # {'churn_probability': 0.23, 'churn_prediction': 0, 'risk_level': 'Low'}
    
    # Batch prediction
    customers = [customer1, customer2, ...]
    batch_result = predictor.predict_batch(customers)
    ```
    """)

with tab3:
    st.markdown("""
    ### Use API
    
    **Start Server**:
    ```bash
    uvicorn main:app --reload
    ```
    
    **Single Prediction**:
    ```python
    import requests
    
    customer = {
        "customerid": "CUST001",
        "age": 35,
        # ... other fields
    }
    
    response = requests.post(
        "http://localhost:8000/predict",
        json=customer
    )
    
    result = response.json()
    print(f"Risk: {result['risk_level']}")
    ```
    
    **Batch Prediction**:
    ```python
    payload = {"customers": [customer1, customer2, ...]}
    
    response = requests.post(
        "http://localhost:8000/predict/batch",
        json=payload
    )
    
    results = response.json()
    print(f"High risk: {results['high_risk_count']}")
    ```
    
    **Check Health**:
    ```bash
    curl http://localhost:8000/health
    ```
    """)

with tab4:
    st.markdown("""
    ### Deploy to AWS ECS
    
    **1. Build & Test Locally**:
    ```bash
    # Test with Docker Compose
    docker-compose up --build
    
    # Test endpoints
    curl http://localhost:8000/health
    ```
    
    **2. Push to ECR**:
    ```bash
    # Build
    docker build -t churn-api:latest .
    
    # Tag
    docker tag churn-api:latest <account>.dkr.ecr.<region>.amazonaws.com/churn-api:latest
    
    # Login
    aws ecr get-login-password --region <region> | \\
      docker login --username AWS --password-stdin <account>.dkr.ecr.<region>.amazonaws.com
    
    # Push
    docker push <account>.dkr.ecr.<region>.amazonaws.com/churn-api:latest
    ```
    
    **3. Deploy to ECS**:
    ```bash
    # Register task definition
    aws ecs register-task-definition \\
      --cli-input-json file://ecs-task-definition.json
    
    # Update service
    aws ecs update-service \\
      --cluster churn-cluster \\
      --service churn-api-service \\
      --force-new-deployment
    ```
    
    **4. Configure Load Balancer**:
    - Target: ECS service
    - Health check: `/health`
    - Port: 8000
    """)

st.markdown("---")

# Project Statistics
st.markdown(
    '<div class="section-header">ğŸ“Š Project Statistics</div>', unsafe_allow_html=True
)

col1, col2, col3 = st.columns(3)

with col1:
    st.markdown("""
    ### ğŸ“ Code Structure
    - **Total Modules**: 8
    - **Total Functions**: 15+
    - **Total Classes**: 3
    - **API Endpoints**: 6
    - **Lines of Code**: ~2000+
    """)

with col2:
    st.markdown("""
    ### ğŸ”§ Features
    - **Raw Features**: 11
    - **Engineered Features**: 10
    - **Total Model Features**: 45+
    - **Categorical Encoded**: 5
    - **Numerical Features**: 6
    """)

with col3:
    st.markdown("""
    ### ğŸš€ Deployment
    - **Docker Stages**: 2
    - **Container Size**: ~1GB
    - **API Response Time**: <100ms
    - **Batch Processing**: Yes
    - **AWS ECS Ready**: Yes
    """)

st.markdown("---")

# Technology Stack
st.markdown(
    '<div class="section-header">ğŸ’» Technology Stack</div>', unsafe_allow_html=True
)

col1, col2 = st.columns(2)

with col1:
    st.markdown("""
    ### **Core ML**
    - ğŸ¤– **XGBoost**: Gradient boosting classifier
    - ğŸ”¬ **Scikit-learn**: Preprocessing, metrics
    - ğŸ¯ **Optuna**: Hyperparameter optimization
    - ğŸ“Š **MLflow**: Experiment tracking
    
    ### **Data Processing**
    - ğŸ¼ **Pandas**: Data manipulation
    - ğŸ”¢ **NumPy**: Numerical operations
    - âœ… **Great Expectations**: Data validation
    
    ### **API & Deployment**
    - âš¡ **FastAPI**: Web framework
    - ğŸ¦„ **Uvicorn**: ASGI server
    - ğŸ³ **Docker**: Containerization
    - â˜ï¸ **AWS ECS**: Orchestration
    """)

with col2:
    st.markdown("""
    ### **Key Libraries**
    ```
    fastapi==0.115.0
    uvicorn==0.30.6
    pydantic==2.9.2
    xgboost==2.1.1
    scikit-learn==1.5.2
    pandas==2.2.3
    numpy==1.26.4
    mlflow==2.16.2
    optuna==4.0.0
    great-expectations==1.1.3
    python-multipart==0.0.12
    joblib==1.4.2
    ```
    
    ### **Development Tools**
    - ğŸ¨ **Streamlit**: Documentation
    - ğŸ““ **Jupyter**: Exploration
    - ğŸ§ª **Pytest**: Testing
    - ğŸ“ **Markdown**: Documentation
    """)

st.markdown("---")

# Best Practices
st.markdown(
    '<div class="section-header">âœ¨ Best Practices Implemented</div>',
    unsafe_allow_html=True,
)

col1, col2 = st.columns(2)

with col1:
    st.markdown("""
    ### **Code Quality**
    - âœ… Modular architecture (src/ structure)
    - âœ… Separation of concerns
    - âœ… Reusable components
    - âœ… Type hints with Pydantic
    - âœ… Error handling
    - âœ… Logging throughout
    
    ### **ML Best Practices**
    - âœ… Feature engineering pipeline
    - âœ… Hyperparameter tuning
    - âœ… Experiment tracking (MLflow)
    - âœ… Model versioning
    - âœ… Data validation
    - âœ… Separate train/test/holdout
    """)

with col2:
    st.markdown("""
    ### **Production Ready**
    - âœ… Health checks (/, /health, /ready)
    - âœ… CORS configuration
    - âœ… Environment-based config
    - âœ… Docker multi-stage builds
    - âœ… Non-root container user
    - âœ… AWS ECS optimized
    
    ### **Documentation**
    - âœ… API examples
    - âœ… Deployment guide
    - âœ… Code comments
    - âœ… This Streamlit app!
    - âœ… README files
    - âœ… Function docstrings
    """)

st.markdown("---")

# Next Steps
st.markdown(
    '<div class="section-header">ğŸ¯ Next Steps & Improvements</div>',
    unsafe_allow_html=True,
)

col1, col2 = st.columns(2)

with col1:
    st.markdown("""
    ### **Potential Enhancements**
    1. ğŸ“Š **A/B Testing Framework**
       - Compare model versions
       - Track business impact
    
    2. ğŸ”„ **CI/CD Pipeline**
       - Automated testing
       - Continuous deployment
    
    3. ğŸ“ˆ **Monitoring & Alerting**
       - Prometheus metrics
       - Grafana dashboards
       - CloudWatch alarms
    
    4. ğŸ¯ **Model Improvements**
       - Try other algorithms
       - Feature selection
       - Ensemble methods
    """)

with col2:
    st.markdown("""
    ### **Production Enhancements**
    1. ğŸ” **Security**
       - API authentication (JWT)
       - Rate limiting
       - Input sanitization
    
    2. ğŸ’¾ **Caching**
       - Redis for predictions
       - Feature store
    
    3. ğŸ“Š **Advanced Features**
       - SHAP explanations
       - Feature importance API
       - What-if analysis
    
    4. ğŸ§ª **Testing**
       - Unit tests
       - Integration tests
       - Load testing
    """)

st.markdown("---")

# Conclusion
st.markdown("""
## ğŸ‰ Conclusion

You now have a **complete end-to-end ML system**:

âœ… **Data Processing** â†’ Clean, validated data  
âœ… **Feature Engineering** â†’ 10 business-driven features  
âœ… **Model Training** â†’ Optimized XGBoost with MLflow  
âœ… **Inference Engine** â†’ Production-ready predictions  
âœ… **REST API** â†’ FastAPI with full endpoints  
âœ… **Deployment** â†’ Docker + AWS ECS ready  
âœ… **Documentation** â†’ Comprehensive Streamlit app  

### **Key Achievements**
- ğŸ“ˆ **95% Recall**: Catches almost all churners
- âš¡ **Fast API**: <100ms response time
- ğŸ³ **Containerized**: Deploy anywhere
- â˜ï¸ **Cloud Ready**: AWS ECS configuration
- ğŸ“š **Well Documented**: Every module explained

### **How to Use This Documentation**
1. **Learn**: Read through each section
2. **Experiment**: Try the code examples
3. **Deploy**: Follow deployment guides
4. **Extend**: Build on this foundation

---

### **Questions?**
- Check the **API Examples** in [API_EXAMPLES.md](API_EXAMPLES.md)
- Review **Deployment Steps** in [DEPLOYMENT.md](DEPLOYMENT.md)
- Test endpoints with **test_api.py**
- Explore notebooks in **notebooks/** folder

---

### **Project Repository**
ğŸ“ Location: `m:/local disk M/machine_learning/E2E_projects/sales-data-churn`

ğŸ¯ **Happy Predicting!** ğŸš€
""")

st.markdown("---")

# Footer
st.markdown(
    """
<div style="text-align: center; padding: 2rem; background-color: #F5F5F5; border-radius: 8px;">
    <h3>ğŸ“ Churn Prediction ML Project</h3>
    <p>Complete End-to-End Machine Learning System</p>
    <p><strong>From Data to Deployment</strong></p>
    <p style="color: #666;">Built with â¤ï¸ using Python, XGBoost, FastAPI, and AWS</p>
</div>
""",
    unsafe_allow_html=True,
)
