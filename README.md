(Project README)

# ğŸ”® Customer Churn Prediction - End-to-End ML Project

A production-ready machine learning system for predicting customer churn with complete MLOps pipeline, including data validation, feature engineering, model training, hyperparameter tuning, REST API, interactive UI, and containerized deployment.

## ğŸ“‹ Table of Contents

- [Features](#-features)
- [Tech Stack](#-tech-stack)
- [Project Architecture](#-project-architecture)
- [Prerequisites](#-prerequisites)
- [Installation](#-installation)
- [Quick Start](#-quick-start)
- [Usage Guide](#-usage-guide)
  - [1. Data Processing](#1-data-processing)
  - [2. Model Training](#2-model-training)
  - [3. Model Inference](#3-model-inference)
  - [4. API Service](#4-api-service)
  - [5. Streamlit UI](#5-streamlit-ui)
  - [6. Docker Deployment](#6-docker-deployment)
- [Project Structure](#-project-structure)
- [Configuration](#-configuration)
- [Testing](#-testing)
- [MLflow Tracking](#-mlflow-tracking)
- [AWS S3 Integration](#-aws-s3-integration)
- [CI/CD Pipeline](#-cicd-pipeline)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)

---

## âœ¨ Features

### Core ML Features
- **Automated Data Processing Pipeline** - Load, validate, and preprocess data
- **Data Validation** - Great Expectations integration for data quality checks
- **Feature Engineering** - Automated feature extraction and transformation
- **Hyperparameter Tuning** - Optuna-based optimization
- **Model Training** - XGBoost with imbalanced-learn techniques
- **Experiment Tracking** - MLflow for model versioning and metrics
- **Model Evaluation** - Comprehensive metrics (Precision, Recall, F1, ROC-AUC)

### Production Features
- **REST API** - FastAPI-based production API with health checks
- **Interactive UI** - Streamlit dashboard for predictions and monitoring
- **Containerization** - Docker and Docker Compose for easy deployment
- **Cloud Integration** - AWS S3 for model storage and retrieval
- **Load Testing** - Locust integration for performance testing
- **Comprehensive Testing** - Unit tests, integration tests, and edge case handling

### MLOps Features
- **CI/CD Ready** - Automated testing and deployment workflows
- **Model Versioning** - MLflow model registry
- **Model Deployment Strategies** - File-based, MLflow, or S3 model loading
- **Monitoring & Health Checks** - API health endpoints and logging
- **Scalable Architecture** - Microservices design with Docker

---

## ğŸ›  Tech Stack

**Machine Learning & Data Science:**
- Python 3.11+
- scikit-learn, XGBoost
- Pandas, NumPy, SciPy
- Optuna (hyperparameter tuning)
- imbalanced-learn
- category-encoders

**MLOps & Experiment Tracking:**
- MLflow (experiment tracking, model registry)
- Great Expectations (data validation)
- Pydantic (data validation)

**API & Web:**
- FastAPI (REST API)
- Uvicorn (ASGI server)
- Streamlit (interactive UI)
- Plotly, Matplotlib, Seaborn (visualization)

**DevOps & Cloud:**
- Docker & Docker Compose
- AWS S3 (model storage)
- boto3 (AWS SDK)
- GitHub Actions (CI/CD)

**Testing & Quality:**
- pytest, pytest-cov
- Locust (load testing)
- pytest-benchmark

---

## ğŸ— Project Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Raw Data      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Validation â”‚ â† Great Expectations
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Preprocessing  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Feature Engineer â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Hyperparameter   â”‚ â† Optuna
â”‚    Tuning       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model Training  â”‚ â† MLflow Tracking
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model Storage  â”‚ â†’ File / MLflow / S3
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         v
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
    v         v
â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API  â”‚  â”‚Streamlit â”‚
â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Prerequisites

- **Python 3.11 or higher**
- **pip** or **conda** package manager
- **Docker** and **Docker Compose** (for containerized deployment)
- **Git** (for cloning the repository)
- **AWS Account** (optional, for S3 integration)

---

## ğŸš€ Installation

### 1. Clone the Repository

```bash
git clone <repository-url>
cd sales-data-churn
```

### 2. Create Virtual Environment

```bash
# Using venv
python -m venv venv

# Activate on Windows
venv\Scripts\activate

# Activate on Linux/Mac
source venv/bin/activate
```

### 3. Install Dependencies

```bash
# Install core dependencies
pip install -r requirements.txt

# Install Streamlit dependencies (if using UI)
pip install -r requirements_streamlit.txt
```

### 4. Verify Installation

```bash
python -c "import sklearn, xgboost, mlflow, fastapi; print('All dependencies installed successfully!')"
```

---

## âš¡ Quick Start

### Option 1: Run Complete Pipeline (Recommended for First Time)

```bash
# Run the full production pipeline
python scripts/run_pipeline.py
```

This will:
1. Load and validate data
2. Preprocess and engineer features
3. Tune hyperparameters with Optuna
4. Train multiple models with MLflow
5. Evaluate and save the best model

### Option 2: Quick Testing Pipeline

```bash
# Fast execution for testing
python scripts/quick_pipeline.py
```

### Option 3: Modular Execution

```bash
# Run specific stages only
python scripts/modular_pipeline.py --stages load preprocess features train

# Run from a specific stage onwards
python scripts/modular_pipeline.py --from features
```

---

## ğŸ“– Usage Guide

### 1. Data Processing

#### Prepare Your Data

Place your data files in the `data/raw/` directory:
- `train.csv` - Training data
- `test.csv` - Test data
- `holdout.csv` - Holdout/validation data

#### Validate Data

```bash
# Validate data quality with Great Expectations
python scripts/validate_train_ge.py --csv data/raw/train.csv \
    --suite great_expectations/expectations/train_suite.yml
```

#### Preprocess Data

```python
from src.data_processing.load import load_data
from src.data_processing.preprocess import preprocess_data

# Load data
train_df = load_data("data/raw/train.csv")

# Preprocess
train_processed = preprocess_data(train_df)
```

### 2. Model Training

#### Using the Pipeline Scripts

```bash
# Full pipeline with all features
python scripts/run_pipeline.py

# Quick pipeline for testing
python scripts/quick_pipeline.py

# Custom configuration
python scripts/modular_pipeline.py --all --n-trials 100 --n-runs 5 --threshold 0.5
```

#### Manual Training

```python
from src.training.optuna_tuning import optimize_hyperparameters
from src.training.mlflow_training import train_with_mlflow

# Optimize hyperparameters
best_params = optimize_hyperparameters(X_train, y_train, n_trials=50)

# Train with MLflow
run_id = train_with_mlflow(X_train, y_train, X_test, y_test, best_params)
```

### 3. Model Inference

#### Single Prediction

```python
from src.inference.inference import create_predictor_from_file

# Load predictor
predictor = create_predictor_from_file("models/churn_model_production.pkl")

# Make prediction
customer_data = {
    "AccountWeeks": 100,
    "ContractRenewal": 1,
    "DataPlan": 1,
    "DataUsage": 2.5,
    # ... other features
}

result = predictor.predict_single(customer_data)
print(f"Churn Probability: {result['churn_probability']:.2%}")
print(f"Prediction: {result['prediction']}")
```

#### Batch Prediction

```python
# Predict on DataFrame
predictions = predictor.predict_batch(test_df)
```

### 4. API Service

#### Start the API Server

```bash
# Development mode
python main.py

# Production mode with Uvicorn
uvicorn main:app --host 0.0.0.0 --port 8000 --workers 4
```

#### Configure Model Source

```bash
# Use file-based model
export MODEL_SOURCE=file
export MODEL_PATH=models/churn_model_production.pkl

# Use MLflow model
export MODEL_SOURCE=mlflow
export MLFLOW_TRACKING_URI=./mlruns
export MLFLOW_EXPERIMENT_NAME=Colab_GPU_Training

# Use S3 model
export MODEL_SOURCE=s3
export S3_BUCKET_NAME=your-bucket-name
export S3_MODEL_NAME=churn_model_production.pkl
export AWS_ACCESS_KEY_ID=your-access-key
export AWS_SECRET_ACCESS_KEY=your-secret-key
```

#### Test API Endpoints

```bash
# Health check
curl http://localhost:8000/health

# Model info
curl http://localhost:8000/model/info

# Single prediction
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"AccountWeeks": 100, "ContractRenewal": 1, "DataPlan": 1, "DataUsage": 2.5, ...}'

# Batch prediction
python test_api.py
```

### 5. Streamlit UI

#### Start Streamlit Dashboard

```bash
# Make sure API is running first
python main.py

# In another terminal, start Streamlit
streamlit run streamlit_app.py
```

#### Access the UI

Open your browser to `http://localhost:8501`

**Features:**
- Single customer prediction
- Batch predictions via CSV upload
- Model information and metrics
- Feature importance visualization
- Interactive explanations

#### Using Documentation Pages

```bash
# Start the comprehensive documentation app
streamlit run streamlit_docs/Home.py
```

This includes:
- ğŸ“ Data Processing Guide
- ğŸ”§ Feature Engineering
- ğŸ“ Model Training
- ğŸ”® Inference Engine
- ğŸŒ API Deployment
- ğŸ“Š Project Overview
- ğŸš€ CI/CD Deployment

### 6. Docker Deployment

#### Build and Run with Docker Compose

```bash
# Build and start all services
docker-compose up --build

# Run in detached mode
docker-compose up -d

# Stop services
docker-compose down
```

This will start:
- **API Service**: `http://localhost:8000`
- **Streamlit UI**: `http://localhost:8501`

#### Build Individual Containers

```bash
# Build API container
docker build -t churn-api -f Dockerfile .

# Build Streamlit container
docker build -t churn-frontend -f Dockerfile.streamlit .

# Run API container
docker run -p 8000:8000 churn-api

# Run Streamlit container
docker run -p 8501:8501 churn-frontend
```

#### Environment Variables for Docker

Create a `.env` file:

```bash
# AWS Configuration
AWS_ACCESS_KEY_ID=your-access-key
AWS_SECRET_ACCESS_KEY=your-secret-key
AWS_REGION=us-east-1
S3_BUCKET_NAME=churn-project-model
S3_MODEL_NAME=churn_model_production.pkl

# Model Configuration
MODEL_SOURCE=s3
MLFLOW_TRACKING_URI=./mlruns
MLFLOW_EXPERIMENT_NAME=Colab_GPU_Training

# API Configuration
DEBUG=false
API_HOST=0.0.0.0
API_PORT=8000
```

---

## ğŸ“‚ Project Structure

```
sales-data-churn/
â”œâ”€â”€ data/                          # Data directory
â”‚   â”œâ”€â”€ raw/                       # Raw data files
â”‚   â”‚   â”œâ”€â”€ train.csv
â”‚   â”‚   â”œâ”€â”€ test.csv
â”‚   â”‚   â””â”€â”€ holdout.csv
â”‚   â””â”€â”€ processed/                 # Processed data files
â”‚       â”œâ”€â”€ train_processed.csv
â”‚       â”œâ”€â”€ test_processed.csv
â”‚       â””â”€â”€ holdout_processed.csv
â”‚
â”œâ”€â”€ src/                           # Source code
â”‚   â”œâ”€â”€ data_processing/           # Data loading and preprocessing
â”‚   â”‚   â”œâ”€â”€ load.py
â”‚   â”‚   â””â”€â”€ preprocess.py
â”‚   â”œâ”€â”€ features/                  # Feature engineering
â”‚   â”‚   â”œâ”€â”€ build_feature.py
â”‚   â”‚   â””â”€â”€ feature_preprocess.py
â”‚   â”œâ”€â”€ training/                  # Model training
â”‚   â”‚   â”œâ”€â”€ optuna_tuning.py
â”‚   â”‚   â”œâ”€â”€ mlflow_training.py
â”‚   â”‚   â””â”€â”€ evaluation.py
â”‚   â”œâ”€â”€ inference/                 # Model inference
â”‚   â”‚   â””â”€â”€ inference.py
â”‚   â””â”€â”€ utils/                     # Utility functions
â”‚       â”œâ”€â”€ data_validator.py
â”‚       â”œâ”€â”€ s3_handler.py
â”‚       â””â”€â”€ upload_to_s3.py
â”‚
â”œâ”€â”€ scripts/                       # Pipeline scripts
â”‚   â”œâ”€â”€ run_pipeline.py            # Full production pipeline
â”‚   â”œâ”€â”€ quick_pipeline.py          # Fast testing pipeline
â”‚   â”œâ”€â”€ modular_pipeline.py        # Flexible stage execution
â”‚   â”œâ”€â”€ colab_pipeline.py          # Google Colab pipeline
â”‚   â”œâ”€â”€ colab_evaluate_holdout.py  # Holdout evaluation
â”‚   â””â”€â”€ README.md                  # Scripts documentation
â”‚
â”œâ”€â”€ tests/                         # Test suite
â”‚   â”œâ”€â”€ test_all.py                # Comprehensive tests
â”‚   â”œâ”€â”€ test_data_processing.py
â”‚   â”œâ”€â”€ test_feature_engineering.py
â”‚   â”œâ”€â”€ test_inference.py
â”‚   â”œâ”€â”€ test_edge_cases.py
â”‚   â””â”€â”€ test_performance.py
â”‚
â”œâ”€â”€ streamlit_docs/                # Streamlit documentation
â”‚   â”œâ”€â”€ Home.py                    # Documentation home
â”‚   â””â”€â”€ pages/                     # Documentation pages
â”‚       â”œâ”€â”€ 1_ğŸ“_Data_Processing.py
â”‚       â”œâ”€â”€ 2_ğŸ”§_Feature_Engineering.py
â”‚       â”œâ”€â”€ 3_ğŸ“_Model_Training.py
â”‚       â”œâ”€â”€ 4_ğŸ”®_Inference_Engine.py
â”‚       â”œâ”€â”€ 5_ğŸŒ_API_Deployment.py
â”‚       â”œâ”€â”€ 6_ğŸ“Š_Project_Overview.py
â”‚       â””â”€â”€ 7_ğŸš€_CI_CD_Deployment.py
â”‚
â”œâ”€â”€ notebooks/                     # Jupyter notebooks
â”‚   â”œâ”€â”€ EDA_and_Feature_Engineering.ipynb
â”‚   â”œâ”€â”€ tree_based_recall_models.ipynb
â”‚   â””â”€â”€ Colab_GPU_Training.ipynb
â”‚
â”œâ”€â”€ mlruns/                        # MLflow tracking data
â”œâ”€â”€ models/                        # Saved models
â”œâ”€â”€ outputs/                       # Evaluation outputs
â”‚
â”œâ”€â”€ main.py                        # FastAPI application
â”œâ”€â”€ streamlit_app.py               # Streamlit UI
â”œâ”€â”€ test_api.py                    # API testing script
â”‚
â”œâ”€â”€ Dockerfile                     # API Docker image
â”œâ”€â”€ Dockerfile.streamlit           # Streamlit Docker image
â”œâ”€â”€ docker-compose.yml             # Docker Compose configuration
â”‚
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ requirements_streamlit.txt     # Streamlit dependencies
â”œâ”€â”€ pyproject.toml                 # Project metadata
â””â”€â”€ README.md                      # This file
```

---

## âš™ï¸ Configuration

### Pipeline Configuration

Edit `scripts/run_pipeline.py` to customize the pipeline:

```python
class PipelineConfig:
    # Data paths
    TRAIN_PATH: str = "data/raw/train.csv"
    TEST_PATH: str = "data/raw/test.csv"
    HOLDOUT_PATH: str = "data/raw/holdout.csv"
    
    # Optuna settings
    N_TRIALS: int = 50
    
    # MLflow settings
    N_RUNS: int = 3
    EXPERIMENT_NAME: str = "Colab_GPU_Training"
    
    # Model settings
    OPTIMIZATION_METRIC: str = "recall"
    THRESHOLD: float = 0.5
    PREPROCESS_STRATEGY: str = "yeo-johnson"
```

### API Configuration

Set environment variables or edit `.env`:

```bash
# Application
APP_NAME=Churn Prediction API
APP_VERSION=1.0.0
DEBUG=false

# Server
API_HOST=0.0.0.0
API_PORT=8000

# Model source (file, mlflow, or s3)
MODEL_SOURCE=mlflow

# File-based model
MODEL_PATH=models/churn_model_production.pkl

# MLflow settings
MLFLOW_TRACKING_URI=./mlruns
MLFLOW_EXPERIMENT_NAME=Colab_GPU_Training
MLFLOW_MODEL_NAME=best_model
MLFLOW_RUN_ID=<run-id>

# S3 settings
S3_BUCKET_NAME=churn-project-model
S3_MODEL_NAME=churn_model_production.pkl
S3_REGION=us-east-1
AWS_ACCESS_KEY_ID=<your-key>
AWS_SECRET_ACCESS_KEY=<your-secret>
```

---

## ğŸ§ª Testing

### Run All Tests

```bash
# Run complete test suite
pytest tests/

# Run with coverage report
pytest tests/ --cov=src --cov-report=html

# Run specific test file
pytest tests/test_inference.py -v

# Run with detailed output
pytest tests/ -v -s
```

### Test Categories

```bash
# Data processing tests
pytest tests/test_data_processing.py

# Feature engineering tests
pytest tests/test_feature_engineering.py

# Inference tests
pytest tests/test_inference.py

# Edge cases
pytest tests/test_edge_cases.py

# Performance tests
pytest tests/test_performance.py
```

### Load Testing

```bash
# Install Locust
pip install locust

# Run load tests (make sure API is running)
locust -f tests/locustfile.py --host=http://localhost:8000
```

Open `http://localhost:8089` to access Locust UI.

---

## ğŸ“Š MLflow Tracking

### Start MLflow UI

```bash
# Start MLflow server
mlflow ui --backend-store-uri ./mlruns --port 5000
```

Access at `http://localhost:5000`

### Track Experiments

```python
import mlflow

# Set tracking URI
mlflow.set_tracking_uri("./mlruns")

# Set experiment
mlflow.set_experiment("Colab_GPU_Training")

# Start run
with mlflow.start_run(run_name="xgboost_run"):
    # Log parameters
    mlflow.log_param("n_estimators", 100)
    
    # Log metrics
    mlflow.log_metric("recall", 0.85)
    
    # Log model
    mlflow.sklearn.log_model(model, "model")
```

### View Experiments

The MLflow UI shows:
- All experiment runs
- Parameters and metrics
- Model artifacts
- Comparison between runs
- Model registry

---

## â˜ï¸ AWS S3 Integration

### Upload Model to S3

```python
from src.utils.upload_to_s3 import upload_model_to_s3

# Upload model
upload_model_to_s3(
    local_file_path="models/churn_model_production.pkl",
    bucket_name="churn-project-model",
    s3_file_name="churn_model_production.pkl"
)
```

### Download Model from S3

```python
from src.utils.s3_handler import S3Handler

# Initialize handler
s3_handler = S3Handler(bucket_name="churn-project-model")

# Download model
s3_handler.download_model("churn_model_production.pkl", "models/downloaded_model.pkl")
```

### Configure AWS Credentials

```bash
# Option 1: Environment variables
export AWS_ACCESS_KEY_ID=your-access-key
export AWS_SECRET_ACCESS_KEY=your-secret-key
export AWS_REGION=us-east-1

# Option 2: AWS CLI
aws configure

# Option 3: .env file
AWS_ACCESS_KEY_ID=your-access-key
AWS_SECRET_ACCESS_KEY=your-secret-key
AWS_REGION=us-east-1
```

---

## ğŸ”„ CI/CD Pipeline

### GitHub Actions Workflow

The project includes CI/CD workflows for:

1. **Automated Testing**
   - Run on every push/PR
   - Execute full test suite
   - Generate coverage reports

2. **Docker Build**
   - Build Docker images
   - Push to container registry
   - Deploy to staging/production

3. **Model Deployment**
   - Validate model performance
   - Upload to S3
   - Update API service

### Setup CI/CD

1. Add secrets to GitHub repository:
   - `AWS_ACCESS_KEY_ID`
   - `AWS_SECRET_ACCESS_KEY`
   - `S3_BUCKET_NAME`

2. Workflow files are in `.github/workflows/`

3. Customize deployment targets in workflow files

---

## ğŸ”§ Troubleshooting

### Common Issues

#### 1. Import Errors

```bash
# Make sure src is in Python path
export PYTHONPATH="${PYTHONPATH}:${PWD}/src"

# Or add to script
import sys
sys.path.insert(0, "src")
```

#### 2. MLflow Tracking Issues

```bash
# Set tracking URI explicitly
export MLFLOW_TRACKING_URI=./mlruns

# Or in Python
import mlflow
mlflow.set_tracking_uri("./mlruns")
```

#### 3. Missing Dependencies

```bash
# Reinstall all dependencies
pip install -r requirements.txt --force-reinstall

# Install specific package
pip install <package-name>
```

#### 4. Docker Permission Issues

```bash
# On Linux, add user to docker group
sudo usermod -aG docker $USER

# Restart Docker service
sudo systemctl restart docker
```

#### 5. API Connection Issues

```bash
# Check if API is running
curl http://localhost:8000/health

# Check Docker logs
docker logs churn-api

# Restart services
docker-compose restart
```

#### 6. Model Loading Errors

```bash
# Verify model file exists
ls -lh models/

# Check model format
python -c "import joblib; model = joblib.load('models/churn_model_production.pkl'); print(type(model))"

# Re-download from S3 if needed
python -c "from src.utils.s3_handler import S3Handler; S3Handler('bucket-name').download_model('model.pkl', 'models/model.pkl')"
```

### Getting Help

- Check the [scripts/README.md](scripts/README.md) for pipeline documentation
- Review logs in `logs/` directory
- Check MLflow UI for experiment details
- Inspect Docker logs: `docker-compose logs`

---

## ğŸ¤ Contributing

Contributions are welcome! Here's how to contribute:

1. **Fork the repository**

2. **Create a feature branch**
   ```bash
   git checkout -b feature/amazing-feature
   ```

3. **Make your changes**
   - Write clean, documented code
   - Follow existing code style
   - Add tests for new features

4. **Run tests**
   ```bash
   pytest tests/ --cov=src
   ```

5. **Commit your changes**
   ```bash
   git commit -m "Add amazing feature"
   ```

6. **Push to your branch**
   ```bash
   git push origin feature/amazing-feature
   ```

7. **Open a Pull Request**

### Development Guidelines

- Follow PEP 8 style guide
- Write docstrings for all functions
- Add type hints where appropriate
- Update tests for changes
- Update documentation

---

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## ğŸ“§ Contact

For questions or support, please open an issue on the GitHub repository.

---

## ğŸ¯ Roadmap

Future enhancements planned:

- [ ] Add more ML models (Random Forest, LightGBM, CatBoost)
- [ ] Implement model explainability (SHAP, LIME)
- [ ] Add real-time monitoring dashboard
- [ ] Kubernetes deployment manifests
- [ ] A/B testing framework
- [ ] Model drift detection
- [ ] Automated retraining pipeline
- [ ] Enhanced feature store integration
- [ ] GraphQL API support
- [ ] Mobile app integration

---

## â­ Acknowledgments

- **MLflow** for experiment tracking
- **Optuna** for hyperparameter optimization
- **FastAPI** for high-performance API
- **Streamlit** for rapid UI development
- **Great Expectations** for data validation
- **XGBoost** for powerful gradient boosting

---

**Made with â¤ï¸ for production-ready ML systems**

